# backend/services/gcs.py
"""
GCS helper utilities for ClariMed (Flask backend)

Features:
- signed_upload_url(...)  -> generate a short-lived signed PUT URL for direct uploads from the browser
- upload_bytes(...)       -> upload bytes from the backend (e.g., ElevenLabs MP3) to GCS
- signed_read_url(...)    -> generate a short-lived signed GET URL for client playback/download
- gcs_blob_path(...)      -> convenience helper to build gs:// URIs

Env:
- GOOGLE_APPLICATION_CREDENTIALS: path to your GCP service account JSON
- GCP_BUCKET_NAME: name of your GCS bucket
"""

from __future__ import annotations

import os
import datetime
from typing import Optional

from google.cloud import storage

# ---- Config ----
BUCKET_NAME = os.getenv("GCP_BUCKET_NAME", "").strip()

if not BUCKET_NAME:
    # We don't raise on import (keeps unit tests/imports happy).
    # Functions will raise helpful errors if called without config.
    pass


# ---- Internals ----
def _client() -> storage.Client:
    """
    Returns a google.cloud.storage Client.
    Authentication uses GOOGLE_APPLICATION_CREDENTIALS automatically.
    """
    try:
        return storage.Client()
    except Exception as e:
        raise RuntimeError(
            "Failed to initialize GCS client. "
            "Ensure GOOGLE_APPLICATION_CREDENTIALS is set and points to a valid service account key."
        ) from e


def _bucket() -> storage.Bucket:
    if not BUCKET_NAME:
        raise RuntimeError("GCP_BUCKET_NAME is not configured.")
    client = _client()
    return client.bucket(BUCKET_NAME)


# ---- Public API ----
def signed_upload_url(
    object_name: str,
    content_type: str = "application/pdf",
    expires_minutes: int = 15,
) -> str:
    """
    Create a V4 signed URL for browser-side direct upload (HTTP PUT).

    Args:
        object_name: path/key inside the bucket (e.g., 'uploads/<session>.pdf')
        content_type: required content-type header that the client must send on PUT
        expires_minutes: URL lifetime

    Returns:
        str: Signed URL
    """
    bucket = _bucket()
    blob = bucket.blob(object_name)
    try:
        url = blob.generate_signed_url(
            version="v4",
            expiration=datetime.timedelta(minutes=expires_minutes),
            method="PUT",
            content_type=content_type,
        )
        return url
    except Exception as e:
        raise RuntimeError(f"Failed to create signed upload URL for {object_name}: {e}") from e


def signed_read_url(
    object_name: str,
    expires_minutes: int = 60,
    response_content_type: Optional[str] = None,
) -> str:
    """
    Create a V4 signed URL for reading/downloading an object (HTTP GET).

    Args:
        object_name: path/key inside the bucket
        expires_minutes: URL lifetime
        response_content_type: optional override for the content-type in response

    Returns:
        str: Signed URL for GET
    """
    bucket = _bucket()
    blob = bucket.blob(object_name)
    params = {}
    if response_content_type:
        params["response_type"] = response_content_type

    try:
        url = blob.generate_signed_url(
            version="v4",
            expiration=datetime.timedelta(minutes=expires_minutes),
            method="GET",
            query_parameters=params or None,
        )
        return url
    except Exception as e:
        raise RuntimeError(f"Failed to create signed read URL for {object_name}: {e}") from e


def upload_bytes(
    object_name: str,
    data: bytes,
    content_type: str,
    make_public: bool = False,
) -> str:
    """
    Upload raw bytes from the backend to GCS (e.g., MP3 generated by TTS).

    Args:
        object_name: path/key inside the bucket (e.g., 'audio/<session>/<idx>.mp3')
        data: bytes to upload
        content_type: MIME type (e.g., 'audio/mpeg', 'application/pdf')
        make_public: if True, sets the blob to public-read (not recommended for PHI)

    Returns:
        str: gs:// URI for the uploaded object
    """
    bucket = _bucket()
    blob = bucket.blob(object_name)
    try:
        blob.upload_from_string(data, content_type=content_type)
        if make_public:
            blob.make_public()
        return gcs_blob_path(object_name)
    except Exception as e:
        raise RuntimeError(f"Failed to upload bytes to {object_name}: {e}") from e


def gcs_blob_path(object_name: str) -> str:
    """
    Returns the canonical gs:// URI for an object in the configured bucket.
    """
    if not BUCKET_NAME:
        raise RuntimeError("GCP_BUCKET_NAME is not configured.")
    return f"gs://{BUCKET_NAME}/{object_name}"